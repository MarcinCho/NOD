{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Laboratorium 5** - Manipulacja danymi przy użyciu **Pandas**"
      ],
      "metadata": {
        "id": "VNIt12I_g7PM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Zepsucie datasetu"
      ],
      "metadata": {
        "id": "oYNBUVH86wSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import kagglehub as kg\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Glssnv95hB1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Przed rozpoczęciem poprawnego należy przygotować dane z typowymi błędami spotykanymi w datasetach."
      ],
      "metadata": {
        "id": "surSjK59h_ZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "num_rows = 10000\n",
        "clean_data = {\n",
        "    'age': np.random.randint(18, 80, num_rows),\n",
        "    'income': np.random.randint(20000, 150000, num_rows),\n",
        "    'education': np.random.choice(['Podstawowa', 'Inżynier', 'Magister', 'Doktor', 'Doktor Habilitowany', 'Profesor'], num_rows),\n",
        "    'city': np.random.choice(['Warszawa', 'Kraków', 'Gdańsk', 'Wrocław', 'Poznań', 'Bielsko-Biała', 'Katowice'], num_rows),\n",
        "    'experience_years': np.random.randint(0, 40, num_rows),\n",
        "    'school_satisfaction': np.random.randint(1, 11, num_rows),\n",
        "    'department': np.random.choice(['IT', 'Ekonomia', 'Marketing', 'HR', 'Finanse', 'Logistyka', 'Filologia', 'Filozofia', 'Fizyka'], num_rows),\n",
        "    'week_working_hours': np.random.randint(20, 60, num_rows),\n",
        "    'commute_time': np.random.randint(5, 120, num_rows),\n",
        "    'performance_score': np.random.uniform(1.0, 5.0, num_rows).round(2),\n",
        "    'remote_days': np.random.randint(0, 6, num_rows)\n",
        "}\n",
        "\n",
        "df_clean = pd.DataFrame(clean_data)\n",
        "df_clean"
      ],
      "metadata": {
        "id": "fwDmdhtOkkTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def psuj_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    seed = 42\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "    df_zepsuty = df.copy()\n",
        "\n",
        "    # 1. BRAKUJĄCE DANE - różne wzorce\n",
        "    numeric_cols = df_zepsuty.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    categorical_cols = df_zepsuty.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    if len(numeric_cols) > 0:\n",
        "        # MCAR - losowe braki w kolumnie numerycznej\n",
        "        col_mcar = random.choice(numeric_cols)\n",
        "        mask_mcar = np.random.random(len(df_zepsuty)) < 0.15\n",
        "        df_zepsuty.loc[mask_mcar, col_mcar] = np.nan\n",
        "\n",
        "        # MAR - braki zależne od innej zmiennej\n",
        "        if len(numeric_cols) > 1:\n",
        "            col_mar = random.choice([c for c in numeric_cols if c != col_mcar])\n",
        "            threshold = df_zepsuty[col_mcar].quantile(0.7) if not df_zepsuty[col_mcar].isna().all() else 0\n",
        "            mask_mar = (df_zepsuty[col_mcar] > threshold) & (np.random.random(len(df_zepsuty)) < 0.25)\n",
        "            df_zepsuty.loc[mask_mar, col_mar] = np.nan\n",
        "\n",
        "        # Kolumna z >60% braków do usunięcia\n",
        "        if len(numeric_cols) > 2:\n",
        "            col_duzo_brakow = random.choice([c for c in numeric_cols if c not in [col_mcar, col_mar] if 'col_mar' in locals()])\n",
        "            mask_duzo = np.random.random(len(df_zepsuty)) < 0.65\n",
        "            df_zepsuty.loc[mask_duzo, col_duzo_brakow] = np.nan\n",
        "\n",
        "    if len(categorical_cols) > 0:\n",
        "        # MNAR - braki w kategorycznej\n",
        "        col_mnar = random.choice(categorical_cols)\n",
        "        unique_vals = df_zepsuty[col_mnar].dropna().unique()\n",
        "        if len(unique_vals) > 1:\n",
        "            # Braki głównie w jednej kategorii\n",
        "            target_val = random.choice(unique_vals)\n",
        "            mask_mnar = (df_zepsuty[col_mnar] == target_val) & (np.random.random(len(df_zepsuty)) < 0.30)\n",
        "            df_zepsuty.loc[mask_mnar, col_mnar] = np.nan\n",
        "\n",
        "    # 2. DUPLIKATY\n",
        "    n_duplicates = int(len(df_zepsuty) * 0.08)  # 8% duplikatów\n",
        "    if n_duplicates > 0:\n",
        "        duplicate_indices = np.random.choice(df_zepsuty.index, size=n_duplicates, replace=True)\n",
        "        df_duplicates = df_zepsuty.loc[duplicate_indices].copy()\n",
        "        df_zepsuty = pd.concat([df_zepsuty, df_duplicates], ignore_index=True)\n",
        "\n",
        "    # 3. OUTLIERY\n",
        "    if len(numeric_cols) > 0:\n",
        "        for i, col in enumerate(numeric_cols[:min(3, len(numeric_cols))]):\n",
        "            if df_zepsuty[col].notna().sum() > 0:\n",
        "                # Różne typy outlierów\n",
        "                n_outliers = int(len(df_zepsuty) * 0.03)\n",
        "                outlier_indices = np.random.choice(df_zepsuty.index, size=n_outliers, replace=False)\n",
        "\n",
        "                mean_val = df_zepsuty[col].mean()\n",
        "                std_val = df_zepsuty[col].std()\n",
        "\n",
        "                if i % 3 == 0:\n",
        "                    # Ekstremalne wartości górne\n",
        "                    df_zepsuty.loc[outlier_indices, col] = mean_val + (4 + np.random.random(n_outliers) * 3) * std_val\n",
        "                elif i % 3 == 1:\n",
        "                    # Ekstremalne wartości dolne\n",
        "                    df_zepsuty.loc[outlier_indices, col] = mean_val - (4 + np.random.random(n_outliers) * 3) * std_val\n",
        "                else:\n",
        "                    # Mix górne i dolne\n",
        "                    half = n_outliers // 2\n",
        "                    df_zepsuty.loc[outlier_indices[:half], col] = mean_val + (4 + np.random.random(half) * 2) * std_val\n",
        "                    df_zepsuty.loc[outlier_indices[half:], col] = mean_val - (4 + np.random.random(n_outliers-half) * 2) * std_val\n",
        "\n",
        "    # 4. NIESPÓJNOŚCI W DANYCH\n",
        "    if len(categorical_cols) > 0:\n",
        "        col_niespojnosc = random.choice(categorical_cols)\n",
        "        unique_vals = df_zepsuty[col_niespojnosc].dropna().unique()\n",
        "\n",
        "        if len(unique_vals) > 0:\n",
        "            # Różne warianty tej samej wartości (whitespace, case)\n",
        "            n_niespojnosci = int(len(df_zepsuty) * 0.05)\n",
        "            niespojnosc_indices = np.random.choice(df_zepsuty[df_zepsuty[col_niespojnosc].notna()].index,\n",
        "                                                   size=min(n_niespojnosci, df_zepsuty[col_niespojnosc].notna().sum()),\n",
        "                                                   replace=False)\n",
        "\n",
        "            for idx in niespojnosc_indices:\n",
        "                val = str(df_zepsuty.loc[idx, col_niespojnosc])\n",
        "                # Losowy wybór problemu\n",
        "                problem = random.choice(['lowercase', 'uppercase', 'whitespace', 'typo'])\n",
        "\n",
        "                if problem == 'lowercase':\n",
        "                    df_zepsuty.loc[idx, col_niespojnosc] = val.lower()\n",
        "                elif problem == 'uppercase':\n",
        "                    df_zepsuty.loc[idx, col_niespojnosc] = val.upper()\n",
        "                elif problem == 'whitespace':\n",
        "                    df_zepsuty.loc[idx, col_niespojnosc] = f\"  {val}  \"\n",
        "                elif problem == 'typo' and len(val) > 2:\n",
        "                    # Zamień losową literę\n",
        "                    pos = random.randint(0, len(val)-1)\n",
        "                    val_list = list(val)\n",
        "                    val_list[pos] = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
        "                    df_zepsuty.loc[idx, col_niespojnosc] = ''.join(val_list)\n",
        "\n",
        "    # 5. WARTOŚCI POZA SENSOWNYM ZAKRESEM\n",
        "    if len(numeric_cols) > 0:\n",
        "        col_nonsense = random.choice([c for c in numeric_cols if c != col_mcar])\n",
        "        nonsense_indices = np.random.choice(df_zepsuty.index, size=int(len(df_zepsuty) * 0.02), replace=False)\n",
        "\n",
        "        # Np. ujemne wartości gdzie powinny być dodatnie\n",
        "        if df_zepsuty[col_nonsense].min() >= 0:\n",
        "            df_zepsuty.loc[nonsense_indices, col_nonsense] = -abs(df_zepsuty.loc[nonsense_indices, col_nonsense])\n",
        "        else:\n",
        "            # Lub bardzo nietypowe wartości\n",
        "            df_zepsuty.loc[nonsense_indices, col_nonsense] = df_zepsuty[col_nonsense].max() * 100\n",
        "\n",
        "    # 6. PROBLEMY Z MULTIKOLINEARNOŚCIĄ - stwórz skorelowane kolumny\n",
        "    if len(numeric_cols) >= 2:\n",
        "        base_col = random.choice(numeric_cols)\n",
        "        new_col_name = f\"{base_col}_correlated\"\n",
        "\n",
        "        # Stwórz kolumnę silnie skorelowaną (r > 0.9)\n",
        "        noise = np.random.normal(0, df_zepsuty[base_col].std() * 0.1, len(df_zepsuty))\n",
        "        df_zepsuty[new_col_name] = df_zepsuty[base_col] * 1.5 + noise + 10\n",
        "\n",
        "    # Wymieszaj kolejność wierszy\n",
        "    df_zepsuty = df_zepsuty.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "    return df_zepsuty"
      ],
      "metadata": {
        "id": "UniSFNz9VeKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_zepsuty = psuj_dataset(df_clean)\n",
        "df_zepsuty"
      ],
      "metadata": {
        "id": "_NafDHupVqtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "df_zepsuty.plot(kind='scatter', x='income', y='experience_years', s=32, alpha=.8)\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "NFropkEk5WPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "def _plot_series(series, series_name, series_index=0):\n",
        "  palette = list(sns.palettes.mpl_palette('Dark2'))\n",
        "  xs = series['commute_time']\n",
        "  ys = series['experience_years']\n",
        "\n",
        "  plt.plot(xs, ys, label=series_name, color=palette[series_index % len(palette)])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5.2), layout='constrained')\n",
        "df_sorted = df_zepsuty.sort_values('commute_time', ascending=True)\n",
        "for i, (series_name, series) in enumerate(df_sorted.groupby('city')):\n",
        "  _plot_series(series, series_name, i)\n",
        "  fig.legend(title='city', bbox_to_anchor=(1, 1), loc='upper left')\n",
        "sns.despine(fig=fig, ax=ax)\n",
        "plt.xlabel('commute_time')\n",
        "_ = plt.ylabel('experience_years')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "kNorXers5VUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "df_zepsuty.plot(kind='scatter', x='age', y='income', s=32, alpha=.8)\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "erIE6qU_5Trg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "df_zepsuty['experience_years'].plot(kind='hist', bins=20, title='experience_years')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "0snqbxSilKLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Naprawianie zepsutego datasetu\n",
        "Teraz można zacząć czyszczenie data frame'u"
      ],
      "metadata": {
        "id": "gdDx6qA3o-W0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.1a. Metoda IQR - identyfikowanie wartości odstających"
      ],
      "metadata": {
        "id": "jab5tlHkiu4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df_zepsuty.select_dtypes(include=[np.number]).columns:\n",
        "  # Obliczanie Qwantyli 1 i 3\n",
        "  Q1 =  df_zepsuty[col].quantile(0.25)\n",
        "  Q3 =  df_zepsuty[col].quantile(0.75)\n",
        "  Q1, Q3\n",
        "\n",
        "  # Obliczanie IQR\n",
        "  IQR = Q3 - Q1\n",
        "  IQR\n",
        "\n",
        "  #Generowanie Wykresu z danych dla konkretnej kolumny\n",
        "\n",
        "  top_line = (Q1- 1.5 * IQR)\n",
        "  bottom_line = (Q3 + 1.5 * IQR)\n",
        "  outliners = df_zepsuty[(df_zepsuty[col] < top_line) | (df_zepsuty[col] > bottom_line)]\n",
        "\n",
        "  normal = df_zepsuty[(df_zepsuty[col] >= top_line) & (df_zepsuty[col] <= bottom_line)]\n",
        "\n",
        "  plt.figure(figsize=(20, 15))\n",
        "\n",
        "  plt.axhspan(top_line, bottom_line, alpha=0.1, color='green')\n",
        "\n",
        "  plt.axhline(y=top_line, color='red', linestyle='--', linewidth=2,\n",
        "            label=f'Dolna granica: {top_line:.2f}')\n",
        "  plt.axhline(y=bottom_line, color='red', linestyle='--', linewidth=2,\n",
        "            label=f'Górna granica: {bottom_line:.2f}')\n",
        "\n",
        "  plt.scatter(normal.index, normal[col], c='blue', s=100,\n",
        "            alpha=0.7, edgecolors='darkblue', linewidth=2,\n",
        "            label=f'Wartości normalne ({len(normal)})', zorder=5)\n",
        "\n",
        "  plt.scatter(outliners.index, outliners[col], c='red', s=100,\n",
        "            alpha=0.7, edgecolors='darkred', linewidth=2,\n",
        "            label=f'Outliery ({len(outliners)})', zorder=5)\n",
        "\n",
        "  plt.xlabel('Indeks', fontsize=12, fontweight='bold')\n",
        "  plt.ylabel(col, fontsize=12, fontweight='bold')\n",
        "  plt.title('Wizualizacja metody IQR dla kolumny ' + col, fontsize=14, fontweight='bold')\n",
        "  plt.grid(True, alpha=0.3, linestyle=':', linewidth=0.5)\n",
        "  plt.legend(loc='best', fontsize=10)\n",
        "\n",
        "  info = f'Q1={Q1:.2f} | Q3={Q3:.2f} | IQR={IQR:.2f} | Outliery: {len(outliners)}'\n",
        "  plt.text(0.5, 0.98, info, transform=plt.gca().transAxes,\n",
        "          fontsize=11, va='top', ha='center',\n",
        "          bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "iMgALiy6cm6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.1.b Metoda Isolation Forest - Analizowanie wartości odstających"
      ],
      "metadata": {
        "id": "yNwMsRZ7wRLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# Dopasowanie modelu Isolation Forest\n",
        "isolation_forest = IsolationForest(contamination=0.05)\n",
        "df_zepsuty[\"outliers\"] = isolation_forest.fit_predict(df_zepsuty[[\"age\", \"income\"]])\n",
        "\n",
        "# Wyswietlenie wartosci odstajacych\n",
        "df_zepsuty[df_zepsuty[\"outliers\"] ==-1]"
      ],
      "metadata": {
        "id": "rg8F5JhxwZBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "fig = px.scatter(df_zepsuty, x=\"age\", y=\"income\", color=\"outliers\", title=\"Wartosci odstajace w danych\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "AomWYz70x0o5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.2. Usuwanie duplikatów"
      ],
      "metadata": {
        "id": "MNDaRT5DtrTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_to_clean = df_zepsuty.copy()"
      ],
      "metadata": {
        "id": "DxAUWNylt6vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_to_clean = df_to_clean.drop_duplicates()"
      ],
      "metadata": {
        "id": "SLEWDHx9ykrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.3. Naprawa typów danych - usuwanie stringów z kolumn numerycznych"
      ],
      "metadata": {
        "id": "x4NP9YsouFAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_columns = df_to_clean.select_dtypes([np.number]).columns\n",
        "\n",
        "for col in numeric_columns:\n",
        "    if col in df_to_clean.columns:\n",
        "        df_to_clean[col] = pd.to_numeric(df_to_clean[col], errors='coerce')"
      ],
      "metadata": {
        "id": "UJc-_CQ8uFWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.4. Usuwanie wartości negatywnych"
      ],
      "metadata": {
        "id": "cmenp52ZuKq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_to_clean = df_to_clean[(df_to_clean[numeric_columns] >= 0).all(axis=1)]"
      ],
      "metadata": {
        "id": "4EhZZsWluLIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.5. Analiza i uzupełnianie brakujących danych"
      ],
      "metadata": {
        "id": "nDuxVciyuNxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(df_to_clean.isnull(), cbar=True, yticklabels=False, cmap='viridis')\n",
        "plt.title('Wizualizacja brakujących danych', fontsize=14, fontweight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TMuICVV7uPQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in numeric_columns:\n",
        "    if col in df_to_clean.columns:\n",
        "        if df_to_clean[col].isnull().sum() > 0:\n",
        "            median_value = df_to_clean[col].median()\n",
        "            df_to_clean[col].fillna(median_value, inplace=True)\n"
      ],
      "metadata": {
        "id": "Qt_qljx_uQcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.6. Resetowanie indeksów"
      ],
      "metadata": {
        "id": "VhQ2PEkGuTLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_to_clean = df_to_clean.reset_index(drop=True)\n",
        "df_to_clean"
      ],
      "metadata": {
        "id": "dJzEvBRCuTtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if df_to_clean.columns.isin(['outliers']).any():\n",
        "  df_to_clean.drop(columns=['outliers'], inplace=True)\n",
        "df_to_clean"
      ],
      "metadata": {
        "id": "tyoU9L859Wtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3. Detekcja wartości odstających - Isolation Forest po wstępnych poprawkach"
      ],
      "metadata": {
        "id": "Z8EcS7pKuW4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "features_for_isolation = ['age', 'income']\n",
        "X_isolation = df_to_clean[features_for_isolation].dropna()\n",
        "\n",
        "isolation_forest = IsolationForest(contamination=0.006, random_state=42)\n",
        "outliers_pred = isolation_forest.fit_predict(X_isolation)\n",
        "\n",
        "df_to_clean['outliers'] = 1\n",
        "df_to_clean.loc[X_isolation.index, 'outliers'] = outliers_pred"
      ],
      "metadata": {
        "id": "hbJHBKNYuXZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_to_clean[df_to_clean['outliers'] == -1]"
      ],
      "metadata": {
        "id": "HfrPp-0IuZnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.1. Wizualizacja wartości odstających"
      ],
      "metadata": {
        "id": "DhzgfYzFubbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter(df_to_clean, x='age', y='income',\n",
        "                 color='outliers',\n",
        "                 color_discrete_map={1: 'blue', -1: 'red'},\n",
        "                 title='Wartości odstające w danych (Isolation Forest)',\n",
        "                 labels={'outliers': 'Typ', 'age': 'Wiek', 'income': 'Przychód'},\n",
        "                 )\n",
        "fig.update_traces(marker=dict(size=8, line=dict(width=1, color='DarkSlateGrey')))\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "HlajT8Ayubrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4. Analiza głównych składowych (PCA)"
      ],
      "metadata": {
        "id": "35VGfAG3ueM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_to_clean.columns"
      ],
      "metadata": {
        "id": "O3h2lPecDIg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_to_clean = df_to_clean.drop(columns=['outliers'])"
      ],
      "metadata": {
        "id": "tRiy6Y1sFXWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "pca_features = ['age', 'income', 'experience_years']\n",
        "X_pca = df_to_clean[pca_features].dropna()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(X_pca)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "principal_components = pca.fit_transform(scaled_data)\n",
        "\n",
        "df_to_clean['PC1'] = np.nan\n",
        "df_to_clean['PC2'] = np.nan\n",
        "df_to_clean.loc[X_pca.index, 'PC1'] = principal_components[:, 0]\n",
        "df_to_clean.loc[X_pca.index, 'PC2'] = principal_components[:, 1]\n",
        "\n",
        "pca.explained_variance_ratio_"
      ],
      "metadata": {
        "id": "b7W-3v0UugQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4.1. Wizualizacja komponentów głównych"
      ],
      "metadata": {
        "id": "tlzfC_SCuiSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.scatter(df_to_clean.dropna(subset=['PC1', 'PC2']),\n",
        "                 x='PC1', y='PC2',\n",
        "                 color='experience_years',\n",
        "                 title='Wizualizacja głównych składowych (PCA)',\n",
        "                 color_continuous_scale='Viridis',\n",
        "                 labels={'experience_years': 'Lata doświadczenia'})\n",
        "fig.update_traces(marker=dict(size=7))\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "ur_dn-TVuirn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4a. Wizualizacja redukcji wymiarowości - t-SNE"
      ],
      "metadata": {
        "id": "pukfkhrlukzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tsne_features = ['age', 'income', 'experience_years']\n",
        "\n",
        "df_to_clean[tsne_features].corr()"
      ],
      "metadata": {
        "id": "PPz3_d7iJPpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "X_tsne = df_to_clean[tsne_features].dropna()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(X_tsne)\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=2000, learning_rate=200)\n",
        "tsne_results = tsne.fit_transform(scaled_data)\n",
        "\n",
        "df_to_clean['tSNE1'] = np.nan\n",
        "df_to_clean['tSNE2'] = np.nan\n",
        "df_to_clean.loc[X_tsne.index, 'tSNE1'] = tsne_results[:, 0]\n",
        "df_to_clean.loc[X_tsne.index, 'tSNE2'] = tsne_results[:, 1]"
      ],
      "metadata": {
        "id": "5Sk4_aWruluN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tsne_plot = df_to_clean.dropna(subset=['tSNE1', 'tSNE2'])\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "scatter = plt.scatter(df_tsne_plot['tSNE1'], df_tsne_plot['tSNE2'],\n",
        "                     c=df_tsne_plot['experience_years'],\n",
        "                     cmap='coolwarm',\n",
        "                     s=50,\n",
        "                     alpha=0.6,\n",
        "                     edgecolors='black',\n",
        "                     linewidth=0.5)\n",
        "plt.colorbar(scatter, label='lata doświadczenia (experience_years)')\n",
        "plt.xlabel('tSNE1', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('tSNE2', fontsize=12, fontweight='bold')\n",
        "plt.title('Wizualizacja t-SNE', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ugxe1Hj0upj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4b. Wizualizacja redukcji wymiarowości - UMAP"
      ],
      "metadata": {
        "id": "WJ7JETsIurjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import umap\n",
        "\n",
        "umap_features = ['age', 'income']\n",
        "X_umap = df_to_clean[umap_features].dropna()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(X_umap)\n",
        "\n",
        "reducer = umap.UMAP(n_neighbors=10, min_dist=0.1, random_state=42)\n",
        "umap_results = reducer.fit_transform(scaled_data)\n",
        "\n",
        "df_to_clean['UMAP1'] = np.nan\n",
        "df_to_clean['UMAP2'] = np.nan\n",
        "df_to_clean.loc[X_umap.index, 'UMAP1'] = umap_results[:, 0]\n",
        "df_to_clean.loc[X_umap.index, 'UMAP2'] = umap_results[:, 1]"
      ],
      "metadata": {
        "id": "8sgBcEPmusUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_umap_plot = df_to_clean.dropna(subset=['UMAP1', 'UMAP2'])\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "scatter = plt.scatter(df_umap_plot['UMAP1'], df_umap_plot['UMAP2'],\n",
        "                     c=df_umap_plot['experience_years'],\n",
        "                     cmap='Spectral',\n",
        "                     s=50,\n",
        "                     alpha=0.6,\n",
        "                     edgecolors='black',\n",
        "                     linewidth=0.5)\n",
        "plt.colorbar(scatter, label='Lata doświadczenia (experience_years)')\n",
        "plt.xlabel('UMAP1', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('UMAP2', fontsize=12, fontweight='bold')\n",
        "plt.title('Wizualizacja UMAP', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dy7UylbduvTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5. Interaktywna analiza zależności"
      ],
      "metadata": {
        "id": "4ndxBaQTuxd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import altair as alt\n",
        "\n",
        "df_altair = df_to_clean.dropna(subset=['age', 'income', 'experience_years']).copy()\n",
        "\n",
        "chart = alt.Chart(df_altair).mark_circle(size=60).encode(\n",
        "    x=alt.X('age:Q', title=' (Wiek)'),\n",
        "    y=alt.Y('experience_years:Q', title='Lata doświadczenia (experience_years)'),\n",
        "    color=alt.Color('income:Q', scale=alt.Scale(scheme='viridis'), title='przychód ($)'),\n",
        "    tooltip=['age', 'income', 'experience_years']\n",
        ").properties(\n",
        "    width=700,\n",
        "    height=500,\n",
        "    title='Interaktywna wizualizacja zależności'\n",
        ").interactive()\n",
        "\n",
        "chart"
      ],
      "metadata": {
        "id": "mA6yZ6PRux_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5a. Analiza macierzy korelacji"
      ],
      "metadata": {
        "id": "q7KxCOQ8u0vT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_features = ['age', 'income', 'experience_years']\n",
        "df_corr = df_to_clean[correlation_features].dropna()\n",
        "correlation_matrix = df_corr.corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm',\n",
        "            center=0, fmt='.3f',\n",
        "            square=True, linewidths=1,\n",
        "            cbar_kws={'label': 'Współczynnik korelacji'})\n",
        "plt.title('Macierz korelacji', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S-cOfQZgu2FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.6. Testy statystyczne - ANOVA"
      ],
      "metadata": {
        "id": "II0gF_uzu2rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.formula.api import ols\n",
        "from statsmodels.stats.anova import anova_lm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df_to_clean['experience_category'] = pd.cut(\n",
        "    df_to_clean['experience_years'],\n",
        "    bins=[0, 5, 15, 50],\n",
        "    labels=['Junior (0-5 lat)', 'Mid (5-15 lat)', 'Senior (15+ lat)'],\n",
        "    include_lowest=True\n",
        ")\n",
        "\n",
        "df_anova = df_to_clean.dropna(subset=['income', 'experience_category']).copy()\n",
        "\n",
        "model = ols('income ~ C(experience_category)', data=df_anova).fit()\n",
        "anova_results = anova_lm(model)\n",
        "\n",
        "p_value = anova_results['PR(>F)'][0]\n",
        "f_statistic = anova_results['F'][0]\n",
        "\n",
        "interpretation = \"Różnice ISTOTNE\" if p_value < 0.05 else \"Różnice NIEISTOTNE\"\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "sns.boxplot(data=df_anova, x='experience_category', y='income', palette='Set2', ax=ax)\n",
        "\n",
        "means = df_anova.groupby('experience_category')['income'].mean()\n",
        "ax.plot(range(len(means)), means, 'D', color='red', markersize=10, label='Średnia', zorder=10)\n",
        "\n",
        "ax.set_xlabel('Kategoria doświadczenia zawodowego', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Roczny dochód (zł)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Porównanie dochodów według kategorii doświadczenia zawodowego', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.xticks(rotation=15, ha='right')\n",
        "\n",
        "textstr = f\"Test ANOVA:\\nF = {f_statistic:.4f}\\np-value = {p_value:.6f}\\n{interpretation}\"\n",
        "props = dict(boxstyle='round', facecolor='wheat', alpha=0.7)\n",
        "ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=11, verticalalignment='top', bbox=props)\n",
        "\n",
        "ax.legend(loc='upper right', fontsize=10)\n",
        "ax.grid(True, alpha=0.3, axis='y', linestyle=':', linewidth=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zLZv4yRiu58X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}